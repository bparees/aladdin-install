# Default values for aladdin-backend
# This is a YAML-formatted file.

# Obs MCP Server configuration
obsMcp:
  enabled: true
  image:
    repository: quay.io/bparees/obs-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 8080
  replicas: 1
  args:
    - "-auth-mode"
    - "header"
  # Prometheus URL for metrics queries
  prometheusUrl: "https://prometheus-k8s.openshift-monitoring.svc:9091"

# Kubernetes MCP Server configuration
# Note: k8s MCP server is only deployed when lightspeedCore.enabled is true
k8sMcp:
  image:
    repository: quay.io/bparees/k8s-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 8080
    targetPort: 8080
  containerPort: 8080
  replicas: 1
  args:
    - "--read-only"
    - "--toolsets"
    - "core"
    - "--log-level"
    - "8"

# NextGenUI MCP Server configuration
nguiMcp:
  enabled: true
  image:
    repository: quay.io/bparees/ngui-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 9200
  containerPort: 9200
  replicas: 1
  # API key for NGUI provider (OpenAI compatible)
  # This should be overridden via --set or values file
  apiKey: ""
  secretName: ngui-llm-api-key
  configMapName: ngui-mcp-config
  env:
    model: "gpt-4.1-nano"
    tools: "generate_ui_component"
    structuredOutputEnabled: "false"

# Lightspeed Core configuration
lightspeedCore:
  enabled: false
  # LLM configuration
  llm:
    # LLM API key (required)
    apiKey: ""
    # Secret name for the API key
    apiKeySecretName: lcore-llm-api-key
  image:
    repository: quay.io/bparees/lightspeed-core
    tag: latest
    pullPolicy: Always
  service:
    port: 8443
  containerPort: 8443
  replicas: 1
  # TLS secret name (auto-generated by OpenShift service serving certs)
  tlsSecretName: lightspeed-core-tls
  # ConfigMap names
  runConfigMapName: llamastack-run
  stackConfigMapName: lightspeed-stack
  # Inference settings
  inference:
    defaultModel: gpt-4o-mini
    defaultProvider: openai
  # MCP servers configuration
  mcpServers:
    - name: obs
      url: "http://genie-obs-mcp-server.openshift-aladdin:8080/mcp"
      authorization_headers:
        Authorization: "kubernetes"    # Uses user's k8s token from request auth
    - name: kube
      url: "http://mcp-kubernetes-svc:8080/mcp"
      authorization_headers:
        Authorization: "kubernetes"    # Uses user's k8s token from request auth
    - name: ngui
      url: "http://ngui-mcp.openshift-aladdin:9200/mcp"
      authorization_headers:
        Authorization: "kubernetes"    # Uses user's k8s token from request auth
  # LlamaStack run.yaml configuration
  models:
    providerId: openai
    providerType: remote::openai
    modelId: gpt-4o-mini
    modelType: llm
    providerModelId: gpt-4o-mini

# OLS Subscription configuration
# Creates a Subscription for the Lightspeed operator
olsSubscription:
  name: lightspeed-operator
  namespace: openshift-lightspeed
  channel: stable
  installPlanApproval: Automatic
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  # startingCSV: lightspeed-operator.v1.0.9
  # OperatorGroup configuration
  # operatorGroupName defaults to namespace if not specified
  operatorGroupName: openshift-lightspeed
  # targetNamespaces defines which namespaces the operator watches
  # Empty list = AllNamespaces mode (cluster-scoped)
  # Single namespace = OwnNamespace mode
  # Multiple namespaces = MultiNamespace mode
  targetNamespaces: ["openshift-lightspeed"]

# OLSConfig configuration
# Creates an OLSConfig custom resource for the Lightspeed operator
olsConfig:
  enabled: true
  llm:
    # LLM API key (required)
    apiKey: ""
    # Secret name for the API key
    apiKeySecretName: ols-llm-api-key
    providers:
      - credentialsSecretName: ols-llm-api-key
        models:
          - gpt-4o-mini
        name: OpenAI
        type: openai
        url: "https://api.openai.com/v1"
  ols:
    defaultModel: gpt-4o-mini
    defaultProvider: OpenAI
  mcpServers:
    obs:
      url: "http://genie-obs-mcp-server.openshift-aladdin:8080/mcp"
    # - name: kube
    #   url: "http://mcp-kubernetes-svc:8080/mcp"
    ngui:
      url: "http://ngui-mcp.openshift-aladdin:9200/mcp"


# ConsolePlugin patch configuration
# Patches an existing ConsolePlugin to point the "ols" proxy to lightspeed-core
# Note: Only applied when lightspeedCore.enabled is true
consolePluginPatch:
  # Name of the ConsolePlugin to patch
  pluginName: genie-web-client
  # Service to point the proxy to
  targetService:
    name: lightspeed-core
    namespace: openshift-aladdin
    port: 8443

