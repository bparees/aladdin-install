# Default values for aladdin-backend
# This is a YAML-formatted file.

aladdin:
  systemPrompt: |
    You are Aladdin, an AI assistant to help users to get all information about the connected Openshift cluster and its metrics.

    Follow these STEPS:
      1. Try to detect the intent of the user query and call appropriate tools to get the data.
      2. Based on the data received from tool calls and user intent, decide if you need to generate a UI component.
        Be proactive in generating UI components to enhance user experience and ALWAYS use the tool "generate_ui_component" to generate UI.
      3. To call tool "generate_ui_component" pass these arguments:
        - data: must be exactly same data received from the tool call. Do not modify anything, just copy data as is.
        - data_type: must be exactly same as tool call name.
      4. If the tool "generate_ui_component" is successfully called, do not generate component in your final text response.
      5. If the tool "generate_ui_component" is not successfull, provide the final answer to the user in markdown format.

# Obs MCP Server configuration
obsMcp:
  enabled: true
  image:
    repository: quay.io/bparees/obs-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 8080
  replicas: 1
  args:
    - "-auth-mode"
    - "header"
  # Prometheus URL for metrics queries
  prometheusUrl: "https://prometheus-k8s.openshift-monitoring.svc:9091"

# Kubernetes MCP Server configuration
# Note: k8s MCP server is only deployed when lightspeedCore.enabled is true
k8sMcp:
  image:
    repository: quay.io/bparees/k8s-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 8080
    targetPort: 8080
  containerPort: 8080
  replicas: 1
  args:
    - "--read-only"
    - "--toolsets"
    - "core"
    - "--log-level"
    - "8"

# NextGenUI MCP Server configuration
nguiMcp:
  enabled: true
  image:
    repository: quay.io/bparees/ngui-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 9200
  containerPort: 9200
  replicas: 1
  # API key for NGUI provider (OpenAI compatible)
  # This should be overridden via --set or values file
  apiKey: ""
  secretName: ngui-llm-api-key
  configMapName: ngui-mcp-config
  env:
    model: "gpt-4.1-nano"
    tools: "generate_ui_component"
    structuredOutputEnabled: "false"

# Lightspeed Core configuration
lightspeedCore:
  enabled: false
  # LLM configuration
  llm:
    # LLM API key (required)
    apiKey: ""
    # Secret name for the API key
    apiKeySecretName: lcore-llm-api-key
  image:
    repository: quay.io/bparees/lightspeed-core
    tag: latest
    pullPolicy: Always
  service:
    port: 8443
  containerPort: 8443
  replicas: 1
  # TLS secret name (auto-generated by OpenShift service serving certs)
  tlsSecretName: lightspeed-core-tls
  # ConfigMap names
  runConfigMapName: llamastack-run
  stackConfigMapName: lightspeed-stack
  # Inference settings
  inference:
    defaultModel: gpt-4o-mini
    defaultProvider: openai
  # MCP servers configuration
  mcpServers:
    - name: obs
      url: "http://genie-obs-mcp-server.openshift-aladdin:8080/mcp"
      authorization_headers:
        Authorization: "kubernetes"    # Uses user's k8s token from request auth
    - name: kube
      url: "http://mcp-kubernetes-svc:8080/mcp"
      authorization_headers:
        Authorization: "kubernetes"    # Uses user's k8s token from request auth
    - name: ngui
      url: "http://ngui-mcp.openshift-aladdin:9200/mcp"
      authorization_headers:
        Authorization: "kubernetes"    # Uses user's k8s token from request auth
  # LlamaStack run.yaml configuration
  models:
    providerId: openai
    providerType: remote::openai
    modelId: gpt-4o-mini
    modelType: llm
    providerModelId: gpt-4o-mini

# OLS Subscription configuration
# Creates a Subscription for the Lightspeed operator
olsSubscription:
  name: lightspeed-operator
  namespace: openshift-lightspeed
  channel: stable
  installPlanApproval: Automatic
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  # startingCSV: lightspeed-operator.v1.0.9
  # OperatorGroup configuration
  # operatorGroupName defaults to namespace if not specified
  operatorGroupName: openshift-lightspeed
  # targetNamespaces defines which namespaces the operator watches
  # Empty list = AllNamespaces mode (cluster-scoped)
  # Single namespace = OwnNamespace mode
  # Multiple namespaces = MultiNamespace mode
  targetNamespaces: ["openshift-lightspeed"]

# OLSConfig configuration
# Creates an OLSConfig custom resource for the Lightspeed operator
olsConfig:
  enabled: true
  llm:
    # LLM API key (required)
    apiKey: ""
    # Secret name for the API key
    apiKeySecretName: ols-llm-api-key
    providers:
      - credentialsSecretName: ols-llm-api-key
        models:
          - gpt-4o-mini
        name: OpenAI
        type: openai
        url: "https://api.openai.com/v1"
  ols:
    defaultModel: gpt-4o-mini
    defaultProvider: OpenAI
  mcpServers:
    obs:
      url: "http://genie-obs-mcp-server.openshift-aladdin:8080/mcp"
    # - name: kube
    #   url: "http://mcp-kubernetes-svc:8080/mcp"
    ngui:
      url: "http://ngui-mcp.openshift-aladdin:9200/mcp"


# ConsolePlugin patch configuration
# Patches an existing ConsolePlugin to point the "ols" proxy to the configured backend service (OLS or LCORE)
consolePluginPatch:
  # Name of the ConsolePlugin to patch
  pluginName: genie-web-client
  # Service to point the proxy to when using lightspeedCore
  lcoreTargetService:
    name: lightspeed-core
    namespace: openshift-aladdin
    port: 8443
  # Service to point the proxy to when using OLS
  olsTargetService:
    name: lightspeed-app-server
    namespace: openshift-lightspeed
    port: 8443

