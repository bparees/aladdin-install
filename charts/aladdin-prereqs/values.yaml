# Default values for aladdin-prereqs
# This is a YAML-formatted file.

# LLM API configuration
llm:
  enabled: true
  # API key for OpenAI (or compatible provider)
  # This should be overridden via --set or values file
  apiKey: ""
  # Secret name for API keys
  secretName: llm-api-key
  # LLM provider settings
  config:
    provider:
      name: OpenAI
      type: openai
      url: "https://api.openai.com/v1"
    model: gpt-4o-mini

# Obs MCP Server configuration
obsMcp:
  enabled: true
  image:
    repository: quay.io/bparees/obs-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 8080
  replicas: 1
  args:
    - "-auth-mode"
    - "header"
  # Prometheus URL for metrics queries
  prometheusUrl: "https://prometheus-k8s.openshift-monitoring.svc:9091"

# Kubernetes MCP Server configuration
k8sMcp:
  enabled: true
  image:
    repository: quay.io/bparees/k8s-mcp
    tag: latest
    pullPolicy: Always
  service:
    port: 8080
    targetPort: 8080
  containerPort: 8080
  replicas: 1
  args:
    - "--read-only"
    - "--toolsets"
    - "core"
    - "--log-level"
    - "8"

# NextGenUI MCP Server configuration
nguiMcp:
  enabled: true
  image:
    repository: quay.io/next-gen-ui/mcp
    tag: dev
    pullPolicy: Always
  service:
    port: 9200
  containerPort: 9200
  replicas: 1
  # API key for NGUI provider (OpenAI compatible)
  # This should be overridden via --set or values file
  apiKey: ""
  secretName: ngui-llm-api-key
  configMapName: ngui-mcp-config
  env:
    model: "gpt-4.1-nano"
    tools: "generate_ui_component"
    structuredOutputEnabled: "false"

# Lightspeed Core configuration
lightspeedCore:
  enabled: true
  image:
    repository: quay.io/bparees/lightspeed-core
    tag: latest
    pullPolicy: Always
  service:
    port: 8443
  containerPort: 8443
  replicas: 1
  # TLS secret name (auto-generated by OpenShift service serving certs)
  tlsSecretName: lightspeed-core-tls
  # ConfigMap names
  runConfigMapName: llamastack-run
  stackConfigMapName: lightspeed-stack
  # Inference settings
  inference:
    defaultModel: gpt-4o-mini
    defaultProvider: openai
  # MCP servers configuration
  mcpServers:
    obs:
      url: "http://genie-obs-mcp-server:8080/mcp"
    kube:
      url: "http://mcp-kubernetes-svc:8080/mcp"
    ngui:
      url: "http://ngui-mcp:9200/mcp"
  # LlamaStack run.yaml configuration
  models:
    providerId: openai
    providerType: remote::openai
    modelId: gpt-4o-mini
    modelType: llm
    providerModelId: gpt-4o-mini

# ConsolePlugin patch configuration
# Patches an existing ConsolePlugin to point the "ols" proxy to lightspeed-core
consolePluginPatch:
  enabled: true
  # Name of the ConsolePlugin to patch
  pluginName: genie-web-client
  # Service to point the proxy to
  targetService:
    name: lightspeed-core
    namespace: openshift-aladdin
    port: 8443

